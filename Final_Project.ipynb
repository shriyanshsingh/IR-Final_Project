{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC9vxWelQBXk"
      },
      "outputs": [],
      "source": [
        "import pyserini\n",
        "from pyserini.search.lucene import LuceneSearcher\n",
        "from pyserini.index import LuceneIndexReader\n",
        "from IPython.core.display import display, HTML\n",
        "from pyserini.search import get_topics\n",
        "import heapq\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import requests\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "def initialize_model(index_path: str) -> Tuple[LuceneSearcher, LuceneIndexReader]:\n",
        "    searcher = LuceneSearcher.from_prebuilt_index(index_path)\n",
        "    reader = LuceneIndexReader.from_prebuilt_index(index_path)\n",
        "    return searcher, reader\n",
        "\n",
        "def generate_token_mapping(docid: str, doc_vec: dict, reader: LuceneIndexReader) -> dict:\n",
        "    doc = reader.doc(docid).raw().lower()\n",
        "    mapping = {}\n",
        "    for word in re.split(r'\\s+', doc):\n",
        "        analyzed = reader.analyze(word)\n",
        "        for t in doc_vec:\n",
        "            if t in analyzed:\n",
        "                word = re.sub(r'\\W+', '', word)\n",
        "                mapping[t] = word\n",
        "    return mapping\n",
        "\n",
        "def get_relevant_terms(query: str, n: int, searcher: LuceneSearcher, reader: LuceneIndexReader, max_terms: int = 5) -> List[Tuple[float, str]]:\n",
        "    hits = searcher.search(query, n)\n",
        "    all_terms = []\n",
        "    for i in hits:\n",
        "        doc_vec = reader.get_document_vector(i.docid)\n",
        "        m = generate_token_mapping(i.docid, doc_vec, reader)\n",
        "\n",
        "        for t, f in doc_vec.items():\n",
        "            tf = f / len(doc_vec)\n",
        "            try:\n",
        "                df = reader.get_term_counts(t)[0]\n",
        "            except:\n",
        "                df = 0\n",
        "            idf = reader.stats()['documents'] / df if df != 0 else 0\n",
        "            tf_idf = tf * idf\n",
        "            all_terms.append((tf_idf, m[t]))\n",
        "\n",
        "    return heapq.nlargest(max_terms, all_terms)\n",
        "\n",
        "def evaluate_follow_up_query(query: str, relevant_terms: List[str], searcher: LuceneSearcher, reader: LuceneIndexReader, k: int = 10) -> float:\n",
        "    enhanced_query = f\"{query} {' '.join(relevant_terms)}\"\n",
        "    hits = searcher.search(enhanced_query, k)\n",
        "    return sum((i + 1) / len(hits) for i in range(len(hits))) / len(hits)\n",
        "\n",
        "def extract_keywords_rake(text: str, n: int = 5) -> List[str]:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_scores = {}\n",
        "\n",
        "    words = word_tokenize(text.lower())\n",
        "    for word in words:\n",
        "        if word not in stop_words and word.isalnum():\n",
        "            if word not in word_scores:\n",
        "                word_scores[word] = 1\n",
        "            else:\n",
        "                word_scores[word] += 1\n",
        "\n",
        "    return sorted(word_scores, key=word_scores.get, reverse=True)[:n]\n",
        "\n",
        "def format_response(query: str, relevant_terms: List[Tuple[float, str]], keywords: List[str]) -> str:\n",
        "    response = f\"Your query: '{query}'\\n\\n\"\n",
        "    response += \"Based on your query, here are some relevant terms that might help refine your search:\\n\"\n",
        "    for i, (score, term) in enumerate(relevant_terms[:5], 1):\n",
        "        response += f\"{i}. {term} (relevance score: {score:.2f})\\n\"\n",
        "\n",
        "    response += \"\\nKeywords extracted from your query:\\n\"\n",
        "    response += \", \".join(keywords)\n",
        "\n",
        "    response += \"\\n\\nWould you like to refine your search using any of these terms or keywords?\"\n",
        "    return response\n",
        "\n",
        "def google_search_api(query: str, api_key: str, num_results: int = 10):\n",
        "    base_url = \"https://www.searchapi.io/api/v1/search\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"num\": num_results,\n",
        "        \"engine\": \"google\",\n",
        "        \"api_key\": api_key\n",
        "    }\n",
        "\n",
        "    response = requests.get(base_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        results = []\n",
        "        if \"organic_results\" in data:\n",
        "            for result in data[\"organic_results\"]:\n",
        "                results.append({\n",
        "                    \"title\": result.get(\"title\", \"No Title\"),\n",
        "                    \"link\": result.get(\"link\", \"No Link\")\n",
        "                })\n",
        "        return results\n",
        "    else:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        return []\n",
        "\n",
        "def run_model_with_google_api(query: str, api_key: str):\n",
        "    print(f\"Running query: {query}\")\n",
        "\n",
        "    search_results = google_search_api(query, api_key)\n",
        "\n",
        "    if search_results:\n",
        "        print(\"\\nTop Search Results:\")\n",
        "        for idx, result in enumerate(search_results, start=1):\n",
        "            print(f\"{idx}. {result['title']} - {result['link']}\")\n",
        "\n",
        "        relevant_terms = [result['title'] for result in search_results]\n",
        "\n",
        "        print(\"\\nRelevant Terms Extracted:\")\n",
        "        for term in relevant_terms[:5]:\n",
        "            print(term)\n",
        "\n",
        "        follow_up_query = f\"{query} {' '.join(relevant_terms[:3])}\"\n",
        "        print(f\"\\nFollow-up Query: {follow_up_query}\")\n",
        "\n",
        "        keywords = extract_keywords_rake(query)\n",
        "        print(\"\\nExtracted Keywords:\")\n",
        "        print(\", \".join(keywords))\n",
        "\n",
        "        formatted_response = format_response(query, [(1.0, term) for term in relevant_terms[:5]], keywords)\n",
        "        print(\"\\nFormatted Response:\")\n",
        "        print(formatted_response)\n",
        "    else:\n",
        "        print(\"No results returned from the Google Search API.\")\n",
        "\n",
        "def conversational_interface(api_key: str):\n",
        "    print(\"Welcome to the Information Retrieval System!\")\n",
        "    print(\"Enter your query or type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nYour query: \")\n",
        "        if query.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        run_model_with_google_api(query, api_key)\n",
        "\n",
        "        refine = input(\"\\nWould you like to refine your search? (yes/no): \")\n",
        "        if refine.lower() == 'yes':\n",
        "            refined_query = input(\"Enter your refined query: \")\n",
        "            run_model_with_google_api(refined_query, api_key)\n",
        "\n",
        "    print(\"Thank you for using the Information Retrieval System!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    API_KEY = API_KEY = \"EKU2jbjfUzRv2uWRTZGSEpyk\"  # Replace with your actual API key\n",
        "    conversational_interface(API_KEY)\n"
      ]
    }
  ]
}